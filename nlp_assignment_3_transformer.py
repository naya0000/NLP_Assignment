# -*- coding: utf-8 -*-
"""[NLP Assignment III] Transformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JPmnDK1bxVNIYOesoXyck9qsMwHWwOA3
"""

!pip install transformers datasets evaluate accelerate simpletransformers

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader

import nltk
from nltk.corpus import movie_reviews
from nltk.tokenize import word_tokenize
nltk.download("movie_reviews")

from collections import defaultdict, Counter
import math
import random

random.seed(0) # Don't change
torch.manual_seed(0)  # Don't change
np.random.seed(0) # Don't change

train_X, train_Y = [], []
test_X, test_Y = [], []

for polarity in movie_reviews.categories():
    label = 0 if polarity == 'neg' else 1
    for fid in movie_reviews.fileids(polarity):
        if random.randrange(5) == 0:
            test_X.append(movie_reviews.raw(fid))
            test_Y.append(label)
        else:
            train_X.append(movie_reviews.raw(fid))
            train_Y.append(label)
print(train_X[0], train_Y[0])

"""# Assignment III
Doing Assignment III by modifying the following code cell.
Your goal is to train a Transformer based model to achieve an Accuracy of 0.85. The higher, the better.

Your solution should be based on Transformer models.
You are free to adjust [the settings](https://simpletransformers.ai/docs/usage/) of the simpletransformers class and [the pre-trained models](https://huggingface.co/models?sort=trending). You can also build the model with [the transformers package](https://huggingface.co/docs/transformers/en/index) for more control in details, but your model has to follow the output format of [the predict() method](https://simpletransformers.ai/docs/classification-models/) of `simpletransformers` for automatic evaluation.

Please note that your code is expected to be executed in the free version of Colab. So please make sure your model and your training procedure is lightweight enough to be successfully executed by our TAs.
"""

import pandas as pd
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments
from transformers import DataCollatorWithPadding
from torch.utils.data import Dataset
from tqdm import tqdm

class CustomDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

class TextClassifier:
    def __init__(self, model_name="bert-base-uncased", num_labels=2):
        self.model_name = model_name
        self.num_labels = num_labels
        self.model = None
        self.tokenizer = None

    def train(self, training_instances, training_labels):
        train_data = list(zip(training_instances, training_labels))
        train_df = pd.DataFrame(train_data, columns=['text', 'labels'])

        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        train_encodings = self.tokenizer(list(train_df['text']), truncation=True, padding=True)
        train_labels = train_df['labels']

        train_dataset = CustomDataset(train_encodings, train_labels)

        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name
        )

        training_args = TrainingArguments(
            output_dir="./results",
            learning_rate=2e-5,
            per_device_train_batch_size=16,
            per_device_eval_batch_size=16,
            num_train_epochs=2,
            weight_decay=0.01,
            push_to_hub=False,
            report_to = "none",
        )

        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
        )

        trainer.train()

    def predict(self, text_inputs, batch_size=16):
        if self.model is None:
            raise ValueError("Model has not been trained yet. Please train the model first.")

        self.model.to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))

        inputs = self.tokenizer(text_inputs, truncation=True, padding=True, return_tensors="pt")

        num_batches = (len(text_inputs) + batch_size - 1) // batch_size

        predictions = []

        for i in tqdm(range(num_batches), desc="Predicting", leave=False):
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, len(text_inputs))

            batch_inputs = {key: value[start_idx:end_idx].to(self.model.device) for key, value in inputs.items()}

            with torch.no_grad():
                outputs = self.model(**batch_inputs)

            batch_predictions = torch.argmax(outputs.logits, dim=1).tolist()
            predictions.extend(batch_predictions)

        return [predictions]

model = TextClassifier()
model.train(train_X, train_Y)

"""## Do Evaluation"""

correct, total = 0, 0

pred_Y = model.predict(test_X)[0]

for prediction, y in zip(pred_Y, test_Y):
    if prediction == y:
        correct += 1
    total += 1

print("%d / %d = %g" % (correct, total, correct / total))